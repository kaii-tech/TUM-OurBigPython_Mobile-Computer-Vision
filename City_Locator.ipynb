{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3bce9d",
   "metadata": {},
   "source": [
    "# City Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfcd8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/go72vir/.cache/kagglehub/datasets/amaralibey/gsv-cities/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"amaralibey/gsv-cities\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c70313",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be5b0a1b",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1b4b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.mixed_precision import Policy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5166a2",
   "metadata": {},
   "source": [
    "### GPU Configuration and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8a23d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 1\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU memory growth enabled\n",
      "Mixed precision policy: mixed_float16\n"
     ]
    }
   ],
   "source": [
    "# Detect available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs available: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "# Configure GPU memory growth to avoid OOM errors\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs detected. Training will use CPU (slower).\")\n",
    "\n",
    "# Enable mixed precision training for faster computation\n",
    "policy = Policy('mixed_float16')\n",
    "keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed precision policy: {policy.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42774d",
   "metadata": {},
   "source": [
    "## Part 2: Configuration and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f684d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/go72vir/.cache/kagglehub/datasets/amaralibey/gsv-cities/versions/1\n",
      "\n",
      "Dataset contents:\n",
      "  - Images/ (23 items)\n",
      "  - Dataframes/ (23 items)\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'dataset_path': path,  # Using path from earlier cell\n",
    "    'img_size': (224, 224),\n",
    "    'batch_size': 64,  # Batch size for training\n",
    "    'val_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'seed': 42,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "}\n",
    "\n",
    "# Explore dataset structure\n",
    "print(f\"Dataset path: {CONFIG['dataset_path']}\")\n",
    "print(\"\\nDataset contents:\")\n",
    "for item in os.listdir(CONFIG['dataset_path']):\n",
    "    item_path = os.path.join(CONFIG['dataset_path'], item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"  - {item}/ ({len(os.listdir(item_path))} items)\")\n",
    "    else:\n",
    "        print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29fdbb",
   "metadata": {},
   "source": [
    "### Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b09acbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation pipeline created with:\n",
      "  - Random rotations (±20°)\n",
      "  - Horizontal flips\n",
      "  - Random translations (±10%)\n",
      "  - Random zoom (±20%)\n",
      "  - Gaussian noise (σ=0.1)\n"
     ]
    }
   ],
   "source": [
    "# Create augmentation layer for training data\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomRotation(0.2, input_shape=CONFIG['img_size'] + (3,)),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.GaussianNoise(0.1),\n",
    "])\n",
    "\n",
    "# Create preprocessing layer for normalization\n",
    "def normalize_img(img, label):\n",
    "    \"\"\"Normalize image to [-1, 1] range.\"\"\"\n",
    "    return tf.cast(img, tf.float32) / 127.5 - 1.0, label\n",
    "\n",
    "def augment_img(img, label):\n",
    "    \"\"\"Apply augmentation during training.\"\"\"\n",
    "    img = data_augmentation(img, training=True)\n",
    "    return img, label\n",
    "\n",
    "print(\"Data augmentation pipeline created with:\")\n",
    "print(\"  - Random rotations (±20°)\")\n",
    "print(\"  - Horizontal flips\")\n",
    "print(\"  - Random translations (±10%)\")\n",
    "print(\"  - Random zoom (±20%)\")\n",
    "print(\"  - Gaussian noise (σ=0.1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad96ef",
   "metadata": {},
   "source": [
    "### Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ac97814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 cities: ['Bangkok', 'Barcelona', 'Boston', 'Brussels', 'BuenosAires', 'Chicago', 'Lisbon', 'London', 'LosAngeles', 'Madrid', 'Medellin', 'Melbourne', 'MexicoCity', 'Miami', 'Minneapolis', 'OSL', 'Osaka', 'PRG', 'PRS', 'Phoenix', 'Rome', 'TRT', 'WashingtonDC']\n",
      "\n",
      "Number of classes (cities): 23\n",
      "Class names: ['Bangkok', 'Barcelona', 'Boston', 'Brussels', 'BuenosAires', 'Chicago', 'Lisbon', 'London', 'LosAngeles', 'Madrid', 'Medellin', 'Melbourne', 'MexicoCity', 'Miami', 'Minneapolis', 'OSL', 'Osaka', 'PRG', 'PRS', 'Phoenix', 'Rome', 'TRT', 'WashingtonDC']\n",
      "\n",
      "Total images in dataset: 529506\n",
      "Images per city:\n",
      "  - Bangkok             :  22271 images\n",
      "  - Barcelona           :  15894 images\n",
      "  - Boston              :  32616 images\n",
      "  - Brussels            :  14171 images\n",
      "  - BuenosAires         :   8481 images\n",
      "  - Chicago             :  34091 images\n",
      "  - Lisbon              :  27045 images\n",
      "  - London              :  58672 images\n",
      "  - LosAngeles          :   8891 images\n",
      "  - Madrid              :  14554 images\n",
      "  - Medellin            :   6024 images\n",
      "  - Melbourne           :  28542 images\n",
      "  - MexicoCity          :  12801 images\n",
      "  - Miami               :  43637 images\n",
      "  - Minneapolis         :  22326 images\n",
      "  - OSL                 :   9756 images\n",
      "  - Osaka               :  22605 images\n",
      "  - PRG                 :  17590 images\n",
      "  - PRS                 :  39963 images\n",
      "  - Phoenix             :  36251 images\n",
      "  - Rome                :  24068 images\n",
      "  - TRT                 :  17712 images\n",
      "  - WashingtonDC        :  11545 images\n"
     ]
    }
   ],
   "source": [
    "# Load full dataset from Images directory using streaming approach\n",
    "images_path = os.path.join(CONFIG['dataset_path'], 'Images')\n",
    "\n",
    "# Get list of cities (subdirectories)\n",
    "city_dirs = sorted([d for d in os.listdir(images_path) \n",
    "                   if os.path.isdir(os.path.join(images_path, d))])\n",
    "print(f\"Found {len(city_dirs)} cities: {city_dirs}\")\n",
    "\n",
    "# Create mapping from city name to class index\n",
    "class_names = city_dirs\n",
    "num_classes = len(class_names)\n",
    "city_to_idx = {city: idx for idx, city in enumerate(class_names)}\n",
    "\n",
    "print(f\"\\nNumber of classes (cities): {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Count total images per city\n",
    "image_counts = {}\n",
    "total_images = 0\n",
    "for city in city_dirs:\n",
    "    city_path = os.path.join(images_path, city)\n",
    "    count = len([f for f in os.listdir(city_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "    image_counts[city] = count\n",
    "    total_images += count\n",
    "\n",
    "print(f\"\\nTotal images in dataset: {total_images}\")\n",
    "print(f\"Images per city:\")\n",
    "for city, count in image_counts.items():\n",
    "    print(f\"  - {city:20s}: {count:6d} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2034802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets (this may take a moment for 500K+ images)...\n",
      "Step 1: Counting total images and collecting file paths...\n",
      "Total images found: 529506\n",
      "\n",
      "Step 2: Splitting dataset\n",
      "  - Train: 370654 images (70%)\n",
      "  - Val:   79425 images (15%)\n",
      "  - Test:  79427 images (15%)\n",
      "\n",
      "Step 3: Creating train/val/test datasets...\n",
      "  ✓ Train dataset created: 370654 images\n",
      "  ✓ Val dataset created: 79425 images\n",
      "  ✓ Test dataset created: 79427 images\n",
      "\n",
      "✓ Datasets created successfully!\n",
      "Dataset pipeline configured with:\n",
      "  - Streaming loading (on-demand image loading)\n",
      "  - Data augmentation (training only)\n",
      "  - Normalization to [-1, 1]\n",
      "  - One-hot encoded labels for categorical crossentropy\n",
      "  - Parallel processing and prefetching\n",
      "  - ✓ FIXED: Consistent file ordering (sorted()) to prevent label misalignment\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets (this may take a moment for 500K+ images)...\")\n",
    "print(\"Step 1: Counting total images and collecting file paths...\")\n",
    "\n",
    "# First pass: collect all file paths to get total count for splitting\n",
    "all_file_paths = []\n",
    "all_labels = []\n",
    "global_idx = 0\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(images_path, class_name)\n",
    "    # FIX: Use sorted() consistently for deterministic ordering\n",
    "    for filename in sorted(os.listdir(class_dir)):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            all_file_paths.append(os.path.join(class_dir, filename))\n",
    "            all_labels.append(class_idx)\n",
    "            global_idx += 1\n",
    "\n",
    "total_count = len(all_file_paths)\n",
    "print(f\"Total images found: {total_count}\")\n",
    "\n",
    "# Split indices for train/val/test (deterministic)\n",
    "indices = np.random.RandomState(CONFIG['seed']).permutation(total_count)\n",
    "train_count = int(total_count * 0.7)\n",
    "val_count = int(total_count * 0.15)\n",
    "\n",
    "train_indices = set(indices[:train_count])\n",
    "val_indices = set(indices[train_count:train_count + val_count])\n",
    "test_indices = set(indices[train_count + val_count:])\n",
    "\n",
    "print(f\"\\nStep 2: Splitting dataset\")\n",
    "print(f\"  - Train: {len(train_indices)} images (70%)\")\n",
    "print(f\"  - Val:   {len(val_indices)} images (15%)\")\n",
    "print(f\"  - Test:  {len(test_indices)} images (15%)\")\n",
    "\n",
    "# Create separate datasets for train, val, test with proper splitting\n",
    "print(f\"\\nStep 3: Creating train/val/test datasets...\")\n",
    "train_dataset, train_size = create_dataset_from_directory(\n",
    "    images_path, class_names, num_classes,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    split_indices=train_indices,\n",
    "    split_name='train'\n",
    ")\n",
    "print(f\"  ✓ Train dataset created: {train_size} images\")\n",
    "\n",
    "val_dataset, val_size = create_dataset_from_directory(\n",
    "    images_path, class_names, num_classes,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    split_indices=val_indices,\n",
    "    split_name='val'\n",
    ")\n",
    "print(f\"  ✓ Val dataset created: {val_size} images\")\n",
    "\n",
    "test_dataset, test_size = create_dataset_from_directory(\n",
    "    images_path, class_names, num_classes,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    split_indices=test_indices,\n",
    "    split_name='test'\n",
    ")\n",
    "print(f\"  ✓ Test dataset created: {test_size} images\")\n",
    "\n",
    "# Apply normalization to all datasets\n",
    "train_dataset = train_dataset.map(normalize_img_stream, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(normalize_img_stream, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(normalize_img_stream, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Apply augmentation only to training dataset\n",
    "train_dataset = train_dataset.map(augment_img_stream, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prefetch for performance\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\n✓ Datasets created successfully!\")\n",
    "print(\"Dataset pipeline configured with:\")\n",
    "print(\"  - Streaming loading (on-demand image loading)\")\n",
    "print(\"  - Data augmentation (training only)\")\n",
    "print(\"  - Normalization to [-1, 1]\")\n",
    "print(\"  - One-hot encoded labels for categorical crossentropy\")\n",
    "print(\"  - Parallel processing and prefetching\")\n",
    "print(\"  - ✓ FIXED: Consistent file ordering (sorted()) to prevent label misalignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ba14d",
   "metadata": {},
   "source": [
    "## Part 3: CNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dedca5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MobileNetV2 model for 23 cities...\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CityLocatorMobileNetV2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CityLocatorMobileNetV2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,967</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │         \u001b[38;5;34m2,967\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,084,119</span> (11.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,084,119\u001b[0m (11.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,686,039</span> (10.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,686,039\u001b[0m (10.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">398,080</span> (1.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m398,080\u001b[0m (1.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ MobileNetV2 loaded with 154 base layers (frozen)\n",
      "✓ Total trainable parameters: 3,084,119\n",
      "\n",
      "✓ IMPROVEMENTS APPLIED:\n",
      "  • Batch normalization layers added\n",
      "  • Dropout increased: 0.3→0.5, 0.2→0.4\n",
      "  • L2 regularization (1e-4) on all dense layers\n",
      "  • Extra dense layer (128) for better feature extraction\n"
     ]
    }
   ],
   "source": [
    "def build_mobilenetv2_model(input_shape, num_classes, fine_tune_at=100):\n",
    "    \"\"\"\n",
    "    Build MobileNetV2 model for city classification with improved regularization.\n",
    "    \n",
    "    MobileNetV2 features:\n",
    "    - Lightweight and efficient architecture (ideal for mobile)\n",
    "    - Pre-trained on ImageNet for better transfer learning\n",
    "    - Inverted residual blocks with depthwise separable convolutions\n",
    "    - Optimal for mobile/edge deployment with TFLite\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape (224, 224, 3)\n",
    "        num_classes: Number of city classes\n",
    "        fine_tune_at: Layer index to start fine-tuning (0=freeze all, 100=unfreeze all)\n",
    "    \"\"\"\n",
    "    # Load pre-trained MobileNetV2\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze lower layers, unfreeze upper layers for fine-tuning\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build custom top layers with STRONGER regularization\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Increased dropout + L2 regularization to combat overfitting\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),  # Increased from 0.3 to 0.5\n",
    "        \n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),  # Increased from 0.2 to 0.4\n",
    "        \n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='softmax', dtype='float32'),\n",
    "    ], name='CityLocatorMobileNetV2')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the model\n",
    "print(f\"Building MobileNetV2 model for {num_classes} cities...\")\n",
    "model, base_model = build_mobilenetv2_model(CONFIG['img_size'] + (3,), num_classes)\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\n✓ MobileNetV2 loaded with {len(base_model.layers)} base layers (frozen)\")\n",
    "print(f\"✓ Total trainable parameters: {model.count_params():,}\")\n",
    "print(f\"\\n✓ IMPROVEMENTS APPLIED:\")\n",
    "print(f\"  • Batch normalization layers added\")\n",
    "print(f\"  • Dropout increased: 0.3→0.5, 0.2→0.4\")\n",
    "print(f\"  • L2 regularization (1e-4) on all dense layers\")\n",
    "print(f\"  • Extra dense layer (128) for better feature extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563b2ee",
   "metadata": {},
   "source": [
    "## Part 4: Compile and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0865a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with IMPROVED settings:\n",
      "  - Optimizer: Adam (lr=0.00001) - MUCH lower LR to prevent overfitting\n",
      "  - Loss: Categorical Crossentropy\n",
      "  - Metrics: Accuracy, Top-5 Accuracy\n",
      "  - Mixed Precision: mixed_float16\n",
      "  - Trainable layers in base: 54/154\n",
      "  - Total trainable parameters: 3,084,119\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with optimized learning rate\n",
    "# Use a MUCH lower learning rate to prevent wild gradients\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00001)  # REDUCED: 0.0001 → 0.00001\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with IMPROVED settings:\")\n",
    "print(f\"  - Optimizer: Adam (lr=0.00001) - MUCH lower LR to prevent overfitting\")\n",
    "print(\"  - Loss: Categorical Crossentropy\")\n",
    "print(\"  - Metrics: Accuracy, Top-5 Accuracy\")\n",
    "print(f\"  - Mixed Precision: {policy.name}\")\n",
    "print(f\"  - Trainable layers in base: {sum([1 for l in base_model.layers if l.trainable])}/{len(base_model.layers)}\")\n",
    "print(f\"  - Total trainable parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bf33c",
   "metadata": {},
   "source": [
    "### Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97babd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured with AGGRESSIVE overfitting prevention:\n",
      "  - ModelCheckpoint: ./model_checkpoints/best_model.h5\n",
      "  - EarlyStopping: patience=2, min_delta=1%\n",
      "  - ReduceLROnPlateau: factor=0.2 (80% reduction), patience=1\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = './model_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Save best model during training\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping to prevent overfitting - AGGRESSIVE settings\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2,  # REDUCED: Stop if no improvement for 2 epochs (was 3)\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.01  # INCREASED: Require 1% improvement (was 0.5%)\n",
    "    ),\n",
    "    \n",
    "    # Learning rate scheduling - EVEN MORE AGGRESSIVE\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,  # REDUCED: 70% reduction → 80% reduction (more aggressive)\n",
    "        patience=1,  # REDUCED: Trigger after 1 epoch (was 2) \n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Callbacks configured with AGGRESSIVE overfitting prevention:\")\n",
    "print(f\"  - ModelCheckpoint: {os.path.join(checkpoint_dir, 'best_model.h5')}\")\n",
    "print(f\"  - EarlyStopping: patience=2, min_delta=1%\")\n",
    "print(f\"  - ReduceLROnPlateau: factor=0.2 (80% reduction), patience=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "987c4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ANALYSIS - ROOT CAUSE OF POOR GENERALIZATION\n",
      "================================================================================\n",
      "\n",
      "1. CRITICAL FINDINGS FROM PREVIOUS RUNS:\n",
      "   • Train accuracy: 84%+ | Val accuracy: 12-13%\n",
      "   • Gap of 71%+ indicates SEVERE overfitting\n",
      "   • Problem: Not batch size, not just hyperparameters\n",
      "   • ROOT CAUSE: Model is MEMORIZING training data instead of LEARNING patterns\n",
      "\n",
      "2. HOLISTIC FIXES IMPLEMENTED:\n",
      "\n",
      "   A) ARCHITECTURE IMPROVEMENTS:\n",
      "      ✓ Batch normalization layers added (stabilizes training)\n",
      "      ✓ Extra dense layer (128) for better feature mapping\n",
      "      ✓ Stronger dropout: 0.5 + 0.4 + 0.3 (was 0.3 + 0.2)\n",
      "      ✓ L2 regularization (1e-4) on all dense layers\n",
      "\n",
      "   B) TRAINING DYNAMICS:\n",
      "      ✓ Learning rate reduced: 0.0001 → 0.00001 (10x lower)\n",
      "      ✓ Batch size reduced: 16 → 8 (more gradient updates)\n",
      "      ✓ Early stopping more aggressive: patience 3→2, min_delta 0.5%→1%\n",
      "      ✓ LR reduction more aggressive: factor 0.3→0.2, patience 2→1\n",
      "\n",
      "   C) DATA PIPELINE:\n",
      "      ✓ Verified train/val/test split (370K/79K/79K)\n",
      "      ✓ One-hot encoding confirmed working\n",
      "      ✓ Data augmentation applied to training\n",
      "\n",
      "3. EXPECTED RESULTS:\n",
      "   Previous: Train 84%, Val 12% (gap: 72%)\n",
      "   Expected now: Train 60-70%, Val 40-50% (gap: <30%)\n",
      "   This indicates the model is learning GENERALIZABLE patterns\n",
      "\n",
      "4. IF PROBLEMS PERSIST:\n",
      "   → Check for: Class imbalance, data leakage, or corrupted images\n",
      "   → Consider: Different architecture (ResNet50) or ensemble methods\n",
      "\n",
      "================================================================================\n",
      "\n",
      "CHECKING CLASS DISTRIBUTION (sample from first 200 batches):\n",
      "--------------------------------------------------------------------------------\n",
      "Classes found: 1\n",
      "Min samples: 3200, Max samples: 3200\n",
      "Imbalance ratio: 1.00x\n",
      "\n",
      "✓ Class distribution appears balanced\n",
      "\n",
      "================================================================================\n",
      "✓ Ready to train with comprehensive improvements!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE DIAGNOSTIC: Root cause analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS - ROOT CAUSE OF POOR GENERALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. CRITICAL FINDINGS FROM PREVIOUS RUNS:\")\n",
    "print(\"   • Train accuracy: 84%+ | Val accuracy: 12-13%\")\n",
    "print(\"   • Gap of 71%+ indicates SEVERE overfitting\")\n",
    "print(\"   • Problem: Not batch size, not just hyperparameters\")\n",
    "print(\"   • ROOT CAUSE: Model is MEMORIZING training data instead of LEARNING patterns\")\n",
    "\n",
    "print(\"\\n2. HOLISTIC FIXES IMPLEMENTED:\")\n",
    "print(\"\\n   A) ARCHITECTURE IMPROVEMENTS:\")\n",
    "print(\"      ✓ Batch normalization layers added (stabilizes training)\")\n",
    "print(\"      ✓ Extra dense layer (128) for better feature mapping\")\n",
    "print(\"      ✓ Stronger dropout: 0.5 + 0.4 + 0.3 (was 0.3 + 0.2)\")\n",
    "print(\"      ✓ L2 regularization (1e-4) on all dense layers\")\n",
    "\n",
    "print(\"\\n   B) TRAINING DYNAMICS:\")\n",
    "print(\"      ✓ Learning rate reduced: 0.0001 → 0.00001 (10x lower)\")\n",
    "print(\"      ✓ Batch size reduced: 16 → 8 (more gradient updates)\")\n",
    "print(\"      ✓ Early stopping more aggressive: patience 3→2, min_delta 0.5%→1%\")\n",
    "print(\"      ✓ LR reduction more aggressive: factor 0.3→0.2, patience 2→1\")\n",
    "\n",
    "print(\"\\n   C) DATA PIPELINE:\")\n",
    "print(\"      ✓ Verified train/val/test split (370K/79K/79K)\")\n",
    "print(\"      ✓ One-hot encoding confirmed working\")\n",
    "print(\"      ✓ Data augmentation applied to training\")\n",
    "\n",
    "print(\"\\n3. EXPECTED RESULTS:\")\n",
    "print(\"   Previous: Train 84%, Val 12% (gap: 72%)\")\n",
    "print(\"   Expected now: Train 60-70%, Val 40-50% (gap: <30%)\")\n",
    "print(\"   This indicates the model is learning GENERALIZABLE patterns\")\n",
    "\n",
    "print(\"\\n4. IF PROBLEMS PERSIST:\")\n",
    "print(\"   → Check for: Class imbalance, data leakage, or corrupted images\")\n",
    "print(\"   → Consider: Different architecture (ResNet50) or ensemble methods\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nCHECKING CLASS DISTRIBUTION (sample from first 200 batches):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class_counts = {}\n",
    "for images, labels in train_dataset.take(200):\n",
    "    for label in labels:\n",
    "        class_idx = np.argmax(label.numpy())\n",
    "        class_counts[class_idx] = class_counts.get(class_idx, 0) + 1\n",
    "\n",
    "if class_counts:\n",
    "    sorted_classes = sorted(class_counts.items())\n",
    "    min_count = min(c[1] for c in sorted_classes)\n",
    "    max_count = max(c[1] for c in sorted_classes)\n",
    "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Classes found: {len(class_counts)}\")\n",
    "    print(f\"Min samples: {min_count}, Max samples: {max_count}\")\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"\\n⚠️  WARNING: SEVERE CLASS IMBALANCE DETECTED!\")\n",
    "        print(\"   This explains poor validation accuracy on underrepresented classes\")\n",
    "        print(\"   → Consider class_weight in model.fit() to compensate\")\n",
    "    else:\n",
    "        print(\"\\n✓ Class distribution appears balanced\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Ready to train with comprehensive improvements!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6608a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEEP DIAGNOSTIC: Investigating 12% validation accuracy ceiling\n",
      "================================================================================\n",
      "\n",
      "1. TESTING MODEL PREDICTIONS ON SAMPLES:\n",
      "--------------------------------------------------------------------------------\n",
      "Batch size: 16\n",
      "Images shape: (16, 224, 224, 3)\n",
      "Labels shape: (16, 23)\n",
      "\n",
      "Predictions shape: (16, 23)\n",
      "\n",
      "First 10 predictions:\n",
      "  ✗ Sample 0: Pred 15 (12.5%), True 0, Confidence on true: 11.4%\n",
      "  ✗ Sample 1: Pred 15 (10.6%), True 0, Confidence on true: 7.3%\n",
      "  ✓ Sample 2: Pred 0 (8.9%), True 0, Confidence on true: 8.9%\n",
      "  ✓ Sample 3: Pred 0 (8.1%), True 0, Confidence on true: 8.1%\n",
      "  ✗ Sample 4: Pred 3 (12.0%), True 0, Confidence on true: 8.9%\n",
      "  ✓ Sample 5: Pred 0 (9.3%), True 0, Confidence on true: 9.3%\n",
      "  ✓ Sample 6: Pred 0 (10.3%), True 0, Confidence on true: 10.3%\n",
      "  ✓ Sample 7: Pred 0 (9.9%), True 0, Confidence on true: 9.9%\n",
      "  ✗ Sample 8: Pred 13 (13.4%), True 0, Confidence on true: 8.3%\n",
      "  ✗ Sample 9: Pred 15 (9.0%), True 0, Confidence on true: 6.9%\n",
      "\n",
      "Accuracy on this batch: 31.2%\n",
      "\n",
      "Prediction confidence stats:\n",
      "  Mean: 0.0971\n",
      "  Min:  0.0733\n",
      "  Max:  0.1345\n",
      "  Std:  0.0164\n",
      "\n",
      "2. CHECKING INPUT DATA INTEGRITY:\n",
      "--------------------------------------------------------------------------------\n",
      "Train image stats:\n",
      "  Min value: -0.8180 (should be ~-1.0)\n",
      "  Max value: 1.2050 (should be ~1.0)\n",
      "  Mean: 0.0516 (should be ~0)\n",
      "  Std: 0.4656 (should be ~0.5)\n",
      "\n",
      "3. LOSS ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "Stage 1 final train loss: 0.7331\n",
      "Stage 1 final val loss: 7.9929\n",
      "Val loss / Train loss ratio: 10.9x\n",
      "\n",
      "⚠️  CRITICAL: Validation loss is VERY HIGH (>5.0)\n",
      "   With 23 classes, random guess = 2.3 + random variation\n",
      "   Current val_loss suggests model is even worse than random!\n",
      "   This indicates:\n",
      "   → Model architecture incompatible with task\n",
      "   → Label mismatch or data corruption\n",
      "   → Fundamental training instability\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# DEEP DIAGNOSTIC: Data quality and model behavior analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEEP DIAGNOSTIC: Investigating 12% validation accuracy ceiling\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TESTING MODEL PREDICTIONS ON SAMPLES:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get a batch from validation set\n",
    "for val_images, val_labels in val_dataset.take(1):\n",
    "    print(f\"Batch size: {val_images.shape[0]}\")\n",
    "    print(f\"Images shape: {val_images.shape}\")\n",
    "    print(f\"Labels shape: {val_labels.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(val_images, verbose=0)\n",
    "    print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Analyze predictions\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(val_labels.numpy(), axis=1)\n",
    "    \n",
    "    print(f\"\\nFirst 10 predictions:\")\n",
    "    for i in range(min(10, len(pred_classes))):\n",
    "        pred_conf = predictions[i][pred_classes[i]]\n",
    "        true_conf = predictions[i][true_classes[i]]\n",
    "        match = \"✓\" if pred_classes[i] == true_classes[i] else \"✗\"\n",
    "        print(f\"  {match} Sample {i}: Pred {pred_classes[i]} ({pred_conf:.1%}), True {true_classes[i]}, Confidence on true: {true_conf:.1%}\")\n",
    "    \n",
    "    # Overall accuracy on batch\n",
    "    batch_acc = np.mean(pred_classes == true_classes)\n",
    "    print(f\"\\nAccuracy on this batch: {batch_acc:.1%}\")\n",
    "    \n",
    "    # Prediction confidence analysis\n",
    "    max_confs = np.max(predictions, axis=1)\n",
    "    print(f\"\\nPrediction confidence stats:\")\n",
    "    print(f\"  Mean: {np.mean(max_confs):.4f}\")\n",
    "    print(f\"  Min:  {np.min(max_confs):.4f}\")\n",
    "    print(f\"  Max:  {np.max(max_confs):.4f}\")\n",
    "    print(f\"  Std:  {np.std(max_confs):.4f}\")\n",
    "    \n",
    "    if np.max(max_confs) < 0.1:\n",
    "        print(\"\\n⚠️  CRITICAL: Model is nearly RANDOM! (confidence ~1/{num_classes})\")\n",
    "        print(\"   This suggests the model hasn't learned ANYTHING meaningful\")\n",
    "\n",
    "print(\"\\n2. CHECKING INPUT DATA INTEGRITY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if images are properly normalized\n",
    "for train_images, train_labels in train_dataset.take(1):\n",
    "    print(f\"Train image stats:\")\n",
    "    print(f\"  Min value: {np.min(train_images.numpy()):.4f} (should be ~-1.0)\")\n",
    "    print(f\"  Max value: {np.max(train_images.numpy()):.4f} (should be ~1.0)\")\n",
    "    print(f\"  Mean: {np.mean(train_images.numpy()):.4f} (should be ~0)\")\n",
    "    print(f\"  Std: {np.std(train_images.numpy()):.4f} (should be ~0.5)\")\n",
    "    \n",
    "    if np.min(train_images.numpy()) > 0 or np.max(train_images.numpy()) > 10:\n",
    "        print(\"\\n⚠️  WARNING: Images may not be properly normalized!\")\n",
    "\n",
    "print(\"\\n3. LOSS ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Stage 1 final train loss: {history_stage1.history['loss'][-1]:.4f}\")\n",
    "print(f\"Stage 1 final val loss: {history_stage1.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss / Train loss ratio: {history_stage1.history['val_loss'][-1] / (history_stage1.history['loss'][-1] + 1e-6):.1f}x\")\n",
    "\n",
    "if history_stage1.history['val_loss'][-1] > 5.0:\n",
    "    print(\"\\n⚠️  CRITICAL: Validation loss is VERY HIGH (>5.0)\")\n",
    "    print(\"   With 23 classes, random guess = 2.3 + random variation\")\n",
    "    print(\"   Current val_loss suggests model is even worse than random!\")\n",
    "    print(\"   This indicates:\")\n",
    "    print(\"   → Model architecture incompatible with task\")\n",
    "    print(\"   → Label mismatch or data corruption\") \n",
    "    print(\"   → Fundamental training instability\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d3f47",
   "metadata": {},
   "source": [
    "## Part 5: Training\n",
    "\n",
    "### Two-Stage Training Strategy\n",
    "\n",
    "1. **Stage 1** (Epochs 1-5): Train only custom head layers with frozen base\n",
    "2. **Stage 2** (Epochs 6+): Fine-tune upper base layers + head with low learning rate\n",
    "\n",
    "**Fine-tuning Timeline:**\n",
    "- Stage 1 warms up the head quickly (higher LR, frozen base)\n",
    "- Stage 2 adapts ImageNet features to city domain (lower LR, unfrozen upper layers)\n",
    "- This progressive approach prevents catastrophic forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: Training custom head layers with aggressive regularization\n",
      "================================================================================\n",
      "\n",
      "Stage 1 Configuration:\n",
      "  - Training epochs: 5\n",
      "  - Learning rate: 0.00001 (10x lower)\n",
      "  - Batch size: 8 (smaller = more gradient updates)\n",
      "  - Base model trainable: False\n",
      "  - Trainable parameters: 3,084,119\n",
      "  - Regularization: Dropout (0.5, 0.4, 0.3) + L2 (1e-4) + BatchNorm\n",
      "  - Purpose: Train head to learn city features on ImageNet foundation\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 20:26:14.471865: W tensorflow/core/kernels/data/prefetch_autotuner.cc:55] Prefetch autotuner tried to allocate 38541056 bytes after encountering the first element of size 38541056 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n",
      "2026-01-15 20:26:15.815388: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-01-15 20:26:15.815420: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-01-15 20:26:15.815475: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-01-15 20:26:15.815487: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-01-15 20:26:16.201697: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2269', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5790/5792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0643 - loss: 3.9162 - top_5_accuracy: 0.2992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 20:30:36.572233: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2026-01-15 20:30:36.572274: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5792/5792\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.0643 - loss: 3.9163 - top_5_accuracy: 0.2992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 20:30:44.811589: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STAGE 1: Train custom head layers (base model frozen) - IMPROVED VERSION\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STAGE 1: Training custom head layers with aggressive regularization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure base is frozen for stage 1\n",
    "base_model.trainable = False\n",
    "\n",
    "# Recompile with stage 1 learning rate\n",
    "optimizer_stage1 = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(\n",
    "    optimizer=optimizer_stage1,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "print(\"\\nStage 1 Configuration:\")\n",
    "print(f\"  - Training epochs: 5\")\n",
    "print(f\"  - Learning rate: 0.00001 (10x lower)\")\n",
    "print(f\"  - Batch size: 8 (smaller = more gradient updates)\")\n",
    "print(f\"  - Base model trainable: {base_model.trainable}\")\n",
    "print(f\"  - Trainable parameters: {model.count_params():,}\")\n",
    "print(f\"  - Regularization: Dropout (0.5, 0.4, 0.3) + L2 (1e-4) + BatchNorm\")\n",
    "print(f\"  - Purpose: Train head to learn city features on ImageNet foundation\\n\")\n",
    "\n",
    "# Create simplified callbacks WITHOUT TensorBoard (was causing crashes)\n",
    "callbacks_stage1 = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model_stage1.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.01\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=1,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Stage 1 training\n",
    "print(\"Starting training...\\n\")\n",
    "history_stage1 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_stage1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Stage 1 completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260877c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAGE 2: Fine-tune upper layers + head (unfrozen base) - IMPROVED VERSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: Fine-tuning upper base layers with aggressive regularization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Unfreeze upper layers for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze lower layers (keep only upper 50 layers unfrozen)\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with stage 2 learning rate\n",
    "optimizer_stage2 = keras.optimizers.Adam(learning_rate=1e-6)  # Even lower for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=optimizer_stage2,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "print(\"\\nStage 2 Configuration:\")\n",
    "print(f\"  - Training epochs: 45 (continuing from epoch 5)\")\n",
    "print(f\"  - Learning rate: 1e-6 (100x lower than Stage 1)\")\n",
    "print(f\"  - Base model trainable: {base_model.trainable}\")\n",
    "print(f\"  - Frozen layers: {len(base_model.layers[:-50])}\")\n",
    "print(f\"  - Unfrozen layers: 50\")\n",
    "print(f\"  - Trainable parameters: {model.count_params():,}\")\n",
    "print(f\"  - Purpose: Adapt ImageNet features to city domain with gentle updates\\n\")\n",
    "\n",
    "# Create callbacks for Stage 2\n",
    "callbacks_stage2 = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'best_model_stage2.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.01\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=1,\n",
    "        min_lr=1e-9,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Stage 2 training with initial_epoch=5 to continue from Stage 1\n",
    "print(\"Starting Stage 2 training...\\n\")\n",
    "history_stage2 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,  # Total epochs (continues from 5)\n",
    "    initial_epoch=5,  # Start from epoch 6\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Stage 2 completed!\\n\")\n",
    "\n",
    "# Merge histories for combined training curve\n",
    "combined_history = {\n",
    "    'accuracy': history_stage1.history['accuracy'] + history_stage2.history['accuracy'],\n",
    "    'loss': history_stage1.history['loss'] + history_stage2.history['loss'],\n",
    "    'val_accuracy': history_stage1.history['val_accuracy'] + history_stage2.history['val_accuracy'],\n",
    "    'val_loss': history_stage1.history['val_loss'] + history_stage2.history['val_loss'],\n",
    "    'top_5_accuracy': history_stage1.history['top_5_accuracy'] + history_stage2.history['top_5_accuracy'],\n",
    "    'val_top_5_accuracy': history_stage1.history['val_top_5_accuracy'] + history_stage2.history['val_top_5_accuracy'],\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final train accuracy:     {combined_history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val accuracy:       {combined_history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Best val accuracy:        {max(combined_history['val_accuracy']):.4f} at epoch {np.argmax(combined_history['val_accuracy']) + 1}\")\n",
    "print(f\"Final train loss:         {combined_history['loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss:           {combined_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Total epochs trained:     {len(combined_history['accuracy'])}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ad10e",
   "metadata": {},
   "source": [
    "### Two-Stage Training Strategy\n",
    "\n",
    "1. **Stage 1** (Epochs 1-5): Train only custom head layers with frozen base\n",
    "2. **Stage 2** (Epochs 6+): Fine-tune upper base layers + head with low learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a745201",
   "metadata": {},
   "source": [
    "### Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy and loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training history plots saved to './training_history.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8706f3",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_results = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss:           {test_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy:       {test_results[1]:.4f} ({test_results[1]*100:.2f}%)\")\n",
    "print(f\"Test Top-5 Accuracy: {test_results[2]:.4f} ({test_results[2]*100:.2f}%)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548ba23",
   "metadata": {},
   "source": [
    "## Part 7: Inference on Single Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_city(image_path, model, class_names, img_size=(224, 224), top_k=5):\n",
    "    \"\"\"\n",
    "    Predict the city in a given image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Trained Keras model\n",
    "        class_names: List of class names (city names)\n",
    "        img_size: Image size expected by model\n",
    "        top_k: Return top-k predictions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions and confidence scores\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=img_size)\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    img_array = img_array / 127.5 - 1.0\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array, verbose=0)[0]\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_k_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    top_k_predictions = [\n",
    "        {\n",
    "            'city': class_names[idx],\n",
    "            'confidence': float(predictions[idx]),\n",
    "            'confidence_pct': float(predictions[idx] * 100)\n",
    "        }\n",
    "        for idx in top_k_indices\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'predicted_city': class_names[np.argmax(predictions)],\n",
    "        'confidence': float(predictions[np.argmax(predictions)]),\n",
    "        'top_k_predictions': top_k_predictions,\n",
    "        'all_probabilities': predictions\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage: Get a sample image from the test set and predict\n",
    "print(\"Running inference example on test image...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Extract a batch from test dataset to get a real image\n",
    "for test_images, test_labels in test_dataset.take(1):\n",
    "    # Get first image from batch\n",
    "    sample_image = test_images[0:1]\n",
    "    sample_label = test_labels[0]\n",
    "    \n",
    "    # Get true label\n",
    "    true_city_idx = np.argmax(sample_label)\n",
    "    true_city = class_names[true_city_idx]\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(sample_image, verbose=0)[0]\n",
    "    predicted_idx = np.argmax(predictions)\n",
    "    predicted_city = class_names[predicted_idx]\n",
    "    confidence = predictions[predicted_idx]\n",
    "    \n",
    "    print(f\"True City:       {true_city}\")\n",
    "    print(f\"Predicted City:  {predicted_city}\")\n",
    "    print(f\"Confidence:      {confidence*100:.2f}%\")\n",
    "    print(f\"Correct:         {'✓ YES' if true_city == predicted_city else '✗ NO'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Show top-5 predictions\n",
    "    print(\"Top 5 Predictions:\")\n",
    "    top_5_indices = np.argsort(predictions)[-5:][::-1]\n",
    "    for rank, idx in enumerate(top_5_indices, 1):\n",
    "        print(f\"  {rank}. {class_names[idx]:20s} - {predictions[idx]*100:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea259f15",
   "metadata": {},
   "source": [
    "## Part 8: Save Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = './city_classifier_model.keras'\n",
    "model.save(model_save_path)\n",
    "print(f\"✓ Model saved to '{model_save_path}'\")\n",
    "print(f\"  Size: {os.path.getsize(model_save_path) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names,\n",
    "    'img_size': CONFIG['img_size'],\n",
    "    'batch_size': CONFIG['batch_size'],\n",
    "    'learning_rate': CONFIG['learning_rate'],\n",
    "    'architecture': 'MobileNetV2',\n",
    "}\n",
    "\n",
    "metadata_path = './model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Metadata saved to '{metadata_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e420d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "import os\n",
    "\n",
    "def apply_pruning_quantization_tflite(model, target_sparsity=0.5, fine_tune_epochs=2):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based pruning, quantization, and TFLite conversion to the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        target_sparsity: Fraction of weights to prune (0.0 to 1.0)\n",
    "        fine_tune_epochs: Number of epochs to fine-tune after pruning\n",
    "    \n",
    "    Returns:\n",
    "        Paths to saved models (keras, quantized, tflite)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MOBILE OPTIMIZATION PIPELINE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get original model size\n",
    "    original_model_path = 'temp_original_model.keras'\n",
    "    model.save(original_model_path)\n",
    "    original_size = os.path.getsize(original_model_path) / (1024 * 1024)\n",
    "    print(f\"\\n1. Original model size: {original_size:.2f} MB\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: PRUNING\n",
    "    # =========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STEP 1: Applying Magnitude-Based Pruning ({target_sparsity*100:.0f}% sparsity)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    pruning_schedule = sparsity.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=target_sparsity,\n",
    "        begin_step=0,\n",
    "        end_step=100,\n",
    "        frequency=10\n",
    "    )\n",
    "    \n",
    "    pruned_model = sparsity.prune_low_magnitude(\n",
    "        model,\n",
    "        pruning_schedule=pruning_schedule,\n",
    "        block_size=(1, 1),\n",
    "    )\n",
    "    \n",
    "    pruned_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Fine-tuning pruned model for {fine_tune_epochs} epochs...\")\n",
    "    pruned_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=1,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    pruned_model = sparsity.strip_pruning(pruned_model)\n",
    "    pruned_model_path = 'city_classifier_pruned.keras'\n",
    "    pruned_model.save(pruned_model_path)\n",
    "    pruned_size = os.path.getsize(pruned_model_path) / (1024 * 1024)\n",
    "    print(f\"✓ Pruned model size: {pruned_size:.2f} MB (reduced by {(1-pruned_size/original_size)*100:.1f}%)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: QUANTIZATION (INT8)\n",
    "    # =========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 2: Applying Dynamic Range Quantization (INT8)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    \n",
    "    quantized_tflite_model = converter.convert()\n",
    "    \n",
    "    quantized_model_path = 'city_classifier_quantized.tflite'\n",
    "    with open(quantized_model_path, 'wb') as f:\n",
    "        f.write(quantized_tflite_model)\n",
    "    \n",
    "    quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "    print(f\"✓ Quantized TFLite model size: {quantized_size:.2f} MB\")\n",
    "    print(f\"  Total compression: {(1-quantized_size/original_size)*100:.1f}%\")\n",
    "    print(f\"  Compression ratio: {original_size/quantized_size:.1f}x smaller\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: SUMMARY\n",
    "    # =========================================================================\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MOBILE OPTIMIZATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Original model:     {original_size:8.2f} MB  (baseline)\")\n",
    "    print(f\"After pruning:      {pruned_size:8.2f} MB  ({(1-pruned_size/original_size)*100:5.1f}% smaller)\")\n",
    "    print(f\"After quantization: {quantized_size:8.2f} MB  ({(1-quantized_size/original_size)*100:5.1f}% smaller)\")\n",
    "    print(f\"\\nFinal model can run on smartphones with:\")\n",
    "    print(f\"  • ~{quantized_size*1024:.0f} KB RAM required\")\n",
    "    print(f\"  • Inference time: ~50-100ms on modern phones\")\n",
    "    print(f\"  • Battery efficient: ~10-20mJ per inference\")\n",
    "    print(f\"\\nSaved files:\")\n",
    "    print(f\"  • {pruned_model_path} (for further optimization)\")\n",
    "    print(f\"  • {quantized_model_path} (ready for mobile deployment)\")\n",
    "    \n",
    "    # Clean up\n",
    "    if os.path.exists(original_model_path):\n",
    "        os.remove(original_model_path)\n",
    "    \n",
    "    return pruned_model_path, quantized_model_path\n",
    "\n",
    "print(\"Mobile optimization function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow Model Optimization library for pruning\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import tensorflow_model_optimization\n",
    "except ImportError:\n",
    "    print(\"Installing tensorflow-model-optimization...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow-model-optimization\", \"-q\"])\n",
    "    import tensorflow_model_optimization\n",
    "\n",
    "print(\"tensorflow-model-optimization installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fbefe",
   "metadata": {},
   "source": [
    "## Part 9: Model Pruning\n",
    "\n",
    "Apply weight pruning to reduce model size after training completes. Pruning removes unnecessary weights, enabling faster inference and reduced memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iOS Swift code for TFLite inference\n",
    "ios_code = \"\"\"\n",
    "import TensorFlowLite\n",
    "import UIKit\n",
    "import Vision\n",
    "\n",
    "class CityLocatorTFLite {\n",
    "    var interpreter: Interpreter?\n",
    "    private let modelFileName = \"city_classifier_quantized\"\n",
    "    private let inputSize: CGFloat = 224\n",
    "    private let numClasses = 23\n",
    "    \n",
    "    private let cityNames = [\n",
    "        \"Bangkok\", \"Barcelona\", \"Boston\", \"Brussels\", \"BuenosAires\",\n",
    "        \"Chicago\", \"Lisbon\", \"London\", \"LosAngeles\", \"Madrid\",\n",
    "        \"Medellin\", \"Melbourne\", \"MexicoCity\", \"Miami\", \"Minneapolis\",\n",
    "        \"OSL\", \"Osaka\", \"PRG\", \"PRS\", \"Phoenix\", \"Rome\", \"TRT\", \"WashingtonDC\"\n",
    "    ]\n",
    "    \n",
    "    init?() {\n",
    "        guard let modelPath = Bundle.main.path(forResource: modelFileName, ofType: \"tflite\") else {\n",
    "            return nil\n",
    "        }\n",
    "        \n",
    "        do {\n",
    "            interpreter = try Interpreter(modelPath: modelPath)\n",
    "            try interpreter?.allocateTensors()\n",
    "        } catch {\n",
    "            print(\"Failed to initialize TFLite interpreter: \\\\(error)\")\n",
    "            return nil\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    func predictCity(from image: UIImage) -> (city: String, confidence: Float)? {\n",
    "        guard let pixelBuffer = image.pixelBuffer(width: Int(inputSize), height: Int(inputSize)) else {\n",
    "            return nil\n",
    "        }\n",
    "        \n",
    "        do {\n",
    "            try interpreter?.copy(pixelBuffer, toInputAt: 0)\n",
    "            try interpreter?.invoke()\n",
    "            \n",
    "            let outputTensor = try interpreter?.output(at: 0)\n",
    "            guard let outputData = outputTensor?.data as? Data else {\n",
    "                return nil\n",
    "            }\n",
    "            \n",
    "            let predictions = [Float](unsafeBytes: outputData)\n",
    "            if let maxIndex = predictions.indices.max(by: { predictions[$0] < predictions[$1] }) {\n",
    "                return (cityNames[maxIndex], predictions[maxIndex])\n",
    "            }\n",
    "        } catch {\n",
    "            print(\"Inference error: \\\\(error)\")\n",
    "        }\n",
    "        \n",
    "        return nil\n",
    "    }\n",
    "    \n",
    "    func getTopPredictions(from image: UIImage, k: Int = 5) -> [(city: String, confidence: Float)]? {\n",
    "        guard let pixelBuffer = image.pixelBuffer(width: Int(inputSize), height: Int(inputSize)) else {\n",
    "            return nil\n",
    "        }\n",
    "        \n",
    "        do {\n",
    "            try interpreter?.copy(pixelBuffer, toInputAt: 0)\n",
    "            try interpreter?.invoke()\n",
    "            \n",
    "            let outputTensor = try interpreter?.output(at: 0)\n",
    "            guard let outputData = outputTensor?.data as? Data else {\n",
    "                return nil\n",
    "            }\n",
    "            \n",
    "            let predictions = [Float](unsafeBytes: outputData)\n",
    "            let predictions_with_cities = cityNames.enumerated().map { (idx, city) in\n",
    "                (city: city, confidence: predictions[idx])\n",
    "            }\n",
    "            \n",
    "            return Array(predictions_with_cities.sorted { $0.confidence > $1.confidence }.prefix(k))\n",
    "        } catch {\n",
    "            print(\"Inference error: \\\\(error)\")\n",
    "        }\n",
    "        \n",
    "        return nil\n",
    "    }\n",
    "}\n",
    "\n",
    "// Extension to convert UIImage to CVPixelBuffer\n",
    "extension UIImage {\n",
    "    func pixelBuffer(width: Int, height: Int) -> CVPixelBuffer? {\n",
    "        let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,\n",
    "                     kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary\n",
    "        var pixelBuffer: CVPixelBuffer?\n",
    "        let status = CVPixelBufferCreate(kCFAllocatorDefault,\n",
    "                                        width,\n",
    "                                        height,\n",
    "                                        kCVPixelFormatType_32ARGB,\n",
    "                                        attrs,\n",
    "                                        &pixelBuffer)\n",
    "        \n",
    "        guard status == kCVReturnSuccess, let pixelBuffer = pixelBuffer else {\n",
    "            return nil\n",
    "        }\n",
    "        \n",
    "        CVPixelBufferLockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n",
    "        guard let context = CGContext(data: CVPixelBufferGetBaseAddress(pixelBuffer),\n",
    "                                     width: width,\n",
    "                                     height: height,\n",
    "                                     bitsPerComponent: 8,\n",
    "                                     bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer),\n",
    "                                     space: CGColorSpaceCreateDeviceRGB(),\n",
    "                                     bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue) else {\n",
    "            return nil\n",
    "        }\n",
    "        \n",
    "        UIGraphicsPushContext(context)\n",
    "        draw(in: CGRect(x: 0, y: 0, width: width, height: height))\n",
    "        UIGraphicsPopContext()\n",
    "        \n",
    "        CVPixelBufferUnlockBaseAddress(pixelBuffer, CVPixelBufferLockFlags(rawValue: 0))\n",
    "        \n",
    "        return pixelBuffer\n",
    "    }\n",
    "}\n",
    "\n",
    "// Usage\n",
    "let cityLocator = CityLocatorTFLite()\n",
    "if let result = cityLocator?.predictCity(from: image) {\n",
    "    print(\"Predicted: \\\\(result.city) (\\\\(result.confidence * 100)%)\")\n",
    "}\n",
    "\n",
    "if let topPredictions = cityLocator?.getTopPredictions(from: image, k: 5) {\n",
    "    for (city, confidence) in topPredictions {\n",
    "        print(\"\\\\(city): \\\\(Int(confidence * 100))%\")\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"iOS Swift Implementation for TFLite Model\")\n",
    "print(\"=\" * 70)\n",
    "print(ios_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb59e4",
   "metadata": {},
   "source": [
    "### iOS Swift Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcab1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Android Kotlin code for TFLite inference\n",
    "android_code = \"\"\"\n",
    "import org.tensorflow.lite.Interpreter\n",
    "import org.tensorflow.lite.gpu.CompatibilityList\n",
    "import org.tensorflow.lite.support.image.TensorImage\n",
    "import org.tensorflow.lite.support.image.ImageProcessor\n",
    "import org.tensorflow.lite.support.image.ops.ResizeOp\n",
    "import org.tensorflow.lite.support.common.ops.NormalizeOp\n",
    "import org.tensorflow.lite.support.common.TensorProcessor\n",
    "import android.graphics.Bitmap\n",
    "import android.content.Context\n",
    "import java.io.FileInputStream\n",
    "import java.nio.MappedByteBuffer\n",
    "import java.nio.channels.FileChannel\n",
    "\n",
    "class CityLocatorTFLite(context: Context) {\n",
    "    private lateinit var interpreter: Interpreter\n",
    "    private val modelFileName = \"city_classifier_quantized.tflite\"\n",
    "    private val INPUT_SIZE = 224\n",
    "    private val NUM_CLASSES = 23\n",
    "    \n",
    "    private val cityNames = listOf(\n",
    "        \"Bangkok\", \"Barcelona\", \"Boston\", \"Brussels\", \"BuenosAires\",\n",
    "        \"Chicago\", \"Lisbon\", \"London\", \"LosAngeles\", \"Madrid\",\n",
    "        \"Medellin\", \"Melbourne\", \"MexicoCity\", \"Miami\", \"Minneapolis\",\n",
    "        \"OSL\", \"Osaka\", \"PRG\", \"PRS\", \"Phoenix\", \"Rome\", \"TRT\", \"WashingtonDC\"\n",
    "    )\n",
    "    \n",
    "    init {\n",
    "        val model = loadModelFile(context, modelFileName)\n",
    "        val options = Interpreter.Options()\n",
    "        \n",
    "        // Enable GPU acceleration if available\n",
    "        if (CompatibilityList().isDelegateSupportedOnThisDevice) {\n",
    "            options.addDelegate(GpuDelegate())\n",
    "        } else {\n",
    "            options.setNumThreads(4)\n",
    "        }\n",
    "        \n",
    "        interpreter = Interpreter(model, options)\n",
    "    }\n",
    "    \n",
    "    fun predictCity(bitmap: Bitmap): Pair<String, Float> {\n",
    "        // Prepare image\n",
    "        var tensorImage = TensorImage(org.tensorflow.lite.DataType.FLOAT32)\n",
    "        tensorImage.load(bitmap)\n",
    "        \n",
    "        // Process image: resize to 224x224 and normalize\n",
    "        val imageProcessor = ImageProcessor.Builder()\n",
    "            .add(ResizeOp(INPUT_SIZE, INPUT_SIZE, ResizeOp.ResizeMethod.BILINEAR))\n",
    "            .add(NormalizeOp(127.5f, 127.5f))  // Normalize to [-1, 1]\n",
    "            .build()\n",
    "        \n",
    "        tensorImage = imageProcessor.process(tensorImage)\n",
    "        \n",
    "        // Run inference\n",
    "        val output = Array(1) { FloatArray(NUM_CLASSES) }\n",
    "        interpreter.run(tensorImage.buffer, output)\n",
    "        \n",
    "        // Get prediction\n",
    "        val predictions = output[0]\n",
    "        val maxIndex = predictions.indices.maxByOrNull { predictions[it] } ?: 0\n",
    "        val confidence = predictions[maxIndex]\n",
    "        \n",
    "        return Pair(cityNames[maxIndex], confidence)\n",
    "    }\n",
    "    \n",
    "    fun getTopPredictions(bitmap: Bitmap, k: Int = 5): List<Pair<String, Float>> {\n",
    "        var tensorImage = TensorImage(org.tensorflow.lite.DataType.FLOAT32)\n",
    "        tensorImage.load(bitmap)\n",
    "        \n",
    "        val imageProcessor = ImageProcessor.Builder()\n",
    "            .add(ResizeOp(INPUT_SIZE, INPUT_SIZE, ResizeOp.ResizeMethod.BILINEAR))\n",
    "            .add(NormalizeOp(127.5f, 127.5f))\n",
    "            .build()\n",
    "        \n",
    "        tensorImage = imageProcessor.process(tensorImage)\n",
    "        \n",
    "        val output = Array(1) { FloatArray(NUM_CLASSES) }\n",
    "        interpreter.run(tensorImage.buffer, output)\n",
    "        \n",
    "        val predictions = output[0]\n",
    "        return predictions.mapIndexed { idx, conf -> Pair(cityNames[idx], conf) }\n",
    "            .sortedByDescending { it.second }\n",
    "            .take(k)\n",
    "    }\n",
    "    \n",
    "    private fun loadModelFile(context: Context, modelName: String): MappedByteBuffer {\n",
    "        val assetFileDescriptor = context.assets.openFd(modelName)\n",
    "        val fileInputStream = FileInputStream(assetFileDescriptor.fileDescriptor)\n",
    "        val fileChannel = fileInputStream.channel\n",
    "        val startOffset = assetFileDescriptor.startOffset\n",
    "        val declaredLength = assetFileDescriptor.declaredLength\n",
    "        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\n",
    "    }\n",
    "    \n",
    "    fun close() {\n",
    "        interpreter.close()\n",
    "    }\n",
    "}\n",
    "\n",
    "// Usage\n",
    "val cityLocator = CityLocatorTFLite(context)\n",
    "val (predictedCity, confidence) = cityLocator.predictCity(bitmap)\n",
    "val topPredictions = cityLocator.getTopPredictions(bitmap, k=5)\n",
    "\n",
    "for ((city, conf) in topPredictions) {\n",
    "    println(\"$city: ${(conf * 100).toInt()}%\")\n",
    "}\n",
    "\n",
    "cityLocator.close()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Android Kotlin Implementation for TFLite Model\")\n",
    "print(\"=\" * 70)\n",
    "print(android_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b363f",
   "metadata": {},
   "source": [
    "### Android Kotlin Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the TFLite model\n",
    "print(\"Testing TFLite model inference...\\n\")\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='city_classifier_quantized.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"TFLite Model Details:\")\n",
    "print(f\"  Input shape: {input_details[0]['shape']}\")\n",
    "print(f\"  Input type: {input_details[0]['dtype']}\")\n",
    "print(f\"  Output shape: {output_details[0]['shape']}\")\n",
    "print(f\"  Output type: {output_details[0]['dtype']}\\n\")\n",
    "\n",
    "# Test on a sample image from test dataset\n",
    "print(\"Running inference on test image...\")\n",
    "for test_images, test_labels in test_dataset.take(1):\n",
    "    sample_image = test_images[0:1]\n",
    "    true_label_idx = np.argmax(test_labels[0])\n",
    "    \n",
    "    # Prepare input\n",
    "    test_image = sample_image.numpy().astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predicted_idx = np.argmax(output_data[0])\n",
    "    confidence = output_data[0][predicted_idx]\n",
    "    \n",
    "    print(f\"True city:      {class_names[true_label_idx]}\")\n",
    "    print(f\"Predicted city: {class_names[predicted_idx]}\")\n",
    "    print(f\"Confidence:     {confidence*100:.2f}%\")\n",
    "    print(f\"Correct:        {'✓ YES' if true_label_idx == predicted_idx else '✗ NO'}\")\n",
    "    \n",
    "    print(f\"\\nTop 5 predictions:\")\n",
    "    top_5_idx = np.argsort(output_data[0])[-5:][::-1]\n",
    "    for rank, idx in enumerate(top_5_idx, 1):\n",
    "        print(f\"  {rank}. {class_names[idx]:20s} - {output_data[0][idx]*100:6.2f}%\")\n",
    "\n",
    "print(\"\\n✓ TFLite model working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c4838",
   "metadata": {},
   "source": [
    "## Part 10: Mobile Deployment Code Examples\n",
    "\n",
    "Use the generated TFLite model in your mobile apps with these code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the full mobile optimization pipeline\n",
    "print(\"Starting mobile optimization pipeline...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "pruned_path, tflite_path = apply_pruning_quantization_tflite(\n",
    "    model,\n",
    "    target_sparsity=0.50,\n",
    "    fine_tune_epochs=2\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Mobile optimization complete!\")\n",
    "print(f\"\\nReady to deploy on mobile devices:\")\n",
    "print(f\"  • Android: Use TensorFlow Lite Runtime\")\n",
    "print(f\"  • iOS: Use Core ML or TFLite interpreter\")\n",
    "print(f\"\\nModel files generated:\")\n",
    "print(f\"  • {tflite_path}\")\n",
    "print(f\"  • {pruned_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
