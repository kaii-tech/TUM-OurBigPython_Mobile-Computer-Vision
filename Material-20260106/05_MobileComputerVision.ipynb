{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fea183-0215-4daf-b95f-b9d8242c52b5",
   "metadata": {},
   "source": [
    "# Mobile Computer Vision\n",
    "\n",
    "In Mobile Computer Vision, ressource-constrained devices are used to run models. In general, there are three ways to make a model ready for mobile deployment:\n",
    "- make it smaller (less weights, less layers)\n",
    "- prune it \n",
    "- quantize the weights\n",
    "\n",
    "Consider [https://xilinx.github.io/Vitis-AI/3.5/html/docs/workflow.html](https://xilinx.github.io/Vitis-AI/3.5/html/docs/workflow.html) for an introduction.\n",
    "\n",
    "For now, we just want to find a small model for the plane detection dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa139dd-0a6b-493d-947e-c4c9c661a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {
    "92dd0fc2-62e4-4cc1-b46f-1480c58a140d.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a132b4e4-b8ce-49de-9726-ddad18c6c61c",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data is available from  [https://api.bgd.ed.tum.de/datasets/gaofen-plane-noplane/](https://api.bgd.ed.tum.de/datasets/gaofen-plane-noplane/). Download it to the folder, where this notebook is placed and unpack it such that a subfolders gaofen is there which looks like this:\n",
    "\n",
    "![image.png](attachment:92dd0fc2-62e4-4cc1-b46f-1480c58a140d.png)\n",
    "\n",
    "Expect errors in the training code, when something is wrong with your paths. Note that the paths are relative to where you start Jupyter notebook, not where your notebook is located. That is, be sure to run jupyter notebook in the right folder. If you are not getting it to work, you can use absolute paths as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628ba70-42d5-4835-a9a1-b3032e510b5f",
   "metadata": {},
   "source": [
    "# Base Model: MobileNet\n",
    "We use the MobileNet model which implements a smart way to reduce multiplications using Depth-Wise convolution. It is a model that provides simplifications such that it can be used on a mobile phone easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cda1b62-9fcf-4e52-843c-79fe472fbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, filters, stride=1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3,3), strides=(stride,stride), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def depthwise_conv_layer(x, filters, stride=1, depth_multiplier=1):\n",
    "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3), strides=(stride,stride), padding='same', depth_multiplier=depth_multiplier, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(1,1), strides=(1,1), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def mobilenet(input_tensor, alpha=1.0, depth_multiplier=1, include_top = False, classes=1000, classifier_activation='softmax'):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        alpha (float, optional): controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier (int, optional): The number of depthwise convolution output channels\n",
    "            for each input channel. Defaults to 1.\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.ZeroPadding2D()(input_tensor)\n",
    "    x = conv_layer(x, int(32 * alpha), 2)\n",
    "    x = depthwise_conv_layer(x, int(64 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(128 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(128 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(256 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(256 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(1024 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(1024 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "\n",
    "#   This are the layer which has been used to train imagenet\n",
    "#   x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#   x = tf.keras.layers.Flatten()(x)\n",
    "#   x = tf.keras.layers.Dense(units= classes, activation=classifier_activation)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8a05a-2dc3-4336-af8b-15549459d7e5",
   "metadata": {},
   "source": [
    "# Building a model out of it\n",
    "This just prvovides a sequence of layers (a quite deep neural network, maybe you need to train on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84461c55-f640-4686-9788-e4a56c74cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "  \"dataset\": \"gaofen\",\n",
    "  \"img_input_shape\": [\n",
    "    224,\n",
    "    224,\n",
    "    3\n",
    "  ],\n",
    "  \"number_of_classes\": 2,\n",
    "  \"lr1\": 1e-4,\n",
    "  \"epochs\": 15\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e89f1b-567a-4e63-a0df-76444b48b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5722 images belonging to 2 classes.\n",
      "Found 2729 images belonging to 2 classes.\n",
      "Found 2492 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_17008\\4282542236.py:58: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "178/178 [==============================] - 459s 2s/step - loss: 0.2217 - accuracy: 0.9125 - val_loss: 1.1922 - val_accuracy: 0.5121\n",
      "Epoch 2/15\n",
      "178/178 [==============================] - 378s 2s/step - loss: 0.0983 - accuracy: 0.9652 - val_loss: 2.4825 - val_accuracy: 0.5118\n",
      "Epoch 3/15\n",
      "178/178 [==============================] - 323s 2s/step - loss: 0.0710 - accuracy: 0.9763 - val_loss: 2.7862 - val_accuracy: 0.5136\n",
      "Epoch 4/15\n",
      "178/178 [==============================] - 318s 2s/step - loss: 0.0525 - accuracy: 0.9830 - val_loss: 1.1160 - val_accuracy: 0.5919\n",
      "Epoch 5/15\n",
      "178/178 [==============================] - 301s 2s/step - loss: 0.0474 - accuracy: 0.9837 - val_loss: 0.0739 - val_accuracy: 0.9768\n",
      "Epoch 6/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0393 - accuracy: 0.9854 - val_loss: 0.0950 - val_accuracy: 0.9761\n",
      "Epoch 7/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.0938 - val_accuracy: 0.9732\n",
      "Epoch 8/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.1110 - val_accuracy: 0.9735\n",
      "Epoch 9/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0210 - accuracy: 0.9926 - val_loss: 0.1095 - val_accuracy: 0.9735\n",
      "Epoch 10/15\n",
      "178/178 [==============================] - 283s 2s/step - loss: 0.0197 - accuracy: 0.9926 - val_loss: 0.1074 - val_accuracy: 0.9695\n",
      "Epoch 11/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0159 - accuracy: 0.9937 - val_loss: 0.1607 - val_accuracy: 0.9710\n",
      "Epoch 12/15\n",
      "178/178 [==============================] - 282s 2s/step - loss: 0.0143 - accuracy: 0.9930 - val_loss: 0.1161 - val_accuracy: 0.9754\n",
      "Epoch 13/15\n",
      "178/178 [==============================] - 999s 6s/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.1978 - val_accuracy: 0.9618\n",
      "Epoch 14/15\n",
      "178/178 [==============================] - 383s 2s/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
      "Epoch 15/15\n",
      "178/178 [==============================] - 384s 2s/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.2024 - val_accuracy: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "new_inputs = tf.keras.layers.Input(shape=cfg[\"img_input_shape\"])\n",
    "x = mobilenet(new_inputs)\n",
    "\n",
    "#conv = tf.keras.Model(new_inputs, x)\n",
    "#conv.load_weights(\"imagenet_mobilenet.h5\") # here, we could inject pre-trained weights, I will provide you some on the dataset page\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "new_outputs = tf.keras.layers.Dense(cfg[\"number_of_classes\"], activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(new_inputs, new_outputs)\n",
    "\n",
    "loss_fn = keras.losses.CategoricalCrossentropy() #from_logits=True\n",
    "optimizer = keras.optimizers.Adam(learning_rate = cfg[\"lr1\"])\n",
    "    \n",
    "use_metrics=['accuracy']\n",
    "# print(model.summary())\n",
    "model.compile(optimizer=optimizer, loss = loss_fn, metrics=use_metrics)\n",
    "# data\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1 / 255.0)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory= \"./gaofen/train/\",\n",
    "    target_size=(cfg[\"img_input_shape\"][0],cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    "    )\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    directory= \"./gaofen/val/\",\n",
    "    target_size=(cfg[\"img_input_shape\"][0],cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    "    )\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    directory= \"./gaofen/test/\",\n",
    "    target_size=(cfg[\"img_input_shape\"][0],cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    "    )\n",
    "\n",
    "# Sometimes, your weights will go NaN (e.g., too large LR)\n",
    "# and it is not automatic that the job ends then. But we want to.\n",
    "    \n",
    "callbacks = [ keras.callbacks.TerminateOnNaN() ]\n",
    "\n",
    "model.fit(train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=cfg[\"epochs\"],\n",
    "    callbacks = callbacks,\n",
    "    verbose=1\n",
    "    )\n",
    "model.save(\"mobilenet-gaofen.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1546c2-ac71-4ea7-a082-198491b61588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 61s 776ms/step - loss: 0.4432 - accuracy: 0.9362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44316813349723816, 0.9361958503723145]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b70ed0-a9dd-4268-b88a-eae06fd8ad23",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The MobileNet architecture provides an architecture deployable to small devices. It is even possible to train it on CPU only in comparably reasonable times for image classification. Detecting planes is possible ;-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd657de-0cf9-4115-bac9-c5b5f1dd4e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
