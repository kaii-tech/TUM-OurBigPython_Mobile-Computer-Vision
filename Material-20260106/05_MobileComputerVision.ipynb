{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fea183-0215-4daf-b95f-b9d8242c52b5",
   "metadata": {},
   "source": [
    "# Mobile Computer Vision\n",
    "\n",
    "In Mobile Computer Vision, ressource-constrained devices are used to run models. In general, there are three ways to make a model ready for mobile deployment:\n",
    "- make it smaller (less weights, less layers)\n",
    "- prune it \n",
    "- quantize the weights\n",
    "\n",
    "Consider [https://xilinx.github.io/Vitis-AI/3.5/html/docs/workflow.html](https://xilinx.github.io/Vitis-AI/3.5/html/docs/workflow.html) for an introduction.\n",
    "\n",
    "For now, we just want to find a small model for the Lab Course Dataset (room classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa139dd-0a6b-493d-947e-c4c9c661a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Klein\\Desktop\\Labcourse\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Num GPUs Available: 0\n",
      "No GPU detected. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure TensorFlow to use GPU in WSL\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Using GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132b4e4-b8ce-49de-9726-ddad18c6c61c",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data is available from the Lab_Course_Dataset folder. The dataset structure is:\n",
    "\n",
    "```\n",
    "Lab_Course_Dataset/\n",
    "├── Lise-Meitner-Str-9_9377/\n",
    "│   └── Lise-Meitner-Str-9_9377/\n",
    "│       ├── Indoor/\n",
    "│       │   ├── 9377_EG/\n",
    "│       │   │   ├── HW_706/, HW_708/, HW_709/\n",
    "│       │   │   └── RM_001/, RM_006/, RM_008/, RM_012/, RM_020/, RM_030/\n",
    "│       │   └── 9377_1OG/\n",
    "│       │       └── HW_716/, HW_718/, HW_719/, RM_126/\n",
    "│       └── Outdoor/\n",
    "│           └── North/, North_East/, South/, South_East/\n",
    "└── Willy-Messerschmitt-5_9387/\n",
    "    └── Willy-Messerschmitt-5_9387/\n",
    "        ├── Indoor/\n",
    "        │   └── HW_701/, RM_001/, RM_006/, RM_009/, RM_010/, RM_011/, RM_012/, RM_019/, RM_020/\n",
    "        └── Outdoor/\n",
    "            └── Main_entrance/\n",
    "```\n",
    "\n",
    "Total: 27 different locations (both indoor rooms and outdoor areas).\n",
    "The script will automatically organize the data into train/val/test splits for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628ba70-42d5-4835-a9a1-b3032e510b5f",
   "metadata": {},
   "source": [
    "# Base Model: MobileNet\n",
    "We use the MobileNet model which implements a smart way to reduce multiplications using Depth-Wise convolution. It is a model that provides simplifications such that it can be used on a mobile phone easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cda1b62-9fcf-4e52-843c-79fe472fbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, filters, stride=1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3,3), strides=(stride,stride), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def depthwise_conv_layer(x, filters, stride=1, depth_multiplier=1):\n",
    "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3), strides=(stride,stride), padding='same', depth_multiplier=depth_multiplier, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(1,1), strides=(1,1), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def mobilenet(input_tensor, alpha=1.0, depth_multiplier=1, include_top = False, classes=1000, classifier_activation='softmax'):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        alpha (float, optional): controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier (int, optional): The number of depthwise convolution output channels\n",
    "            for each input channel. Defaults to 1.\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.ZeroPadding2D()(input_tensor)\n",
    "    x = conv_layer(x, int(32 * alpha), 2)\n",
    "    x = depthwise_conv_layer(x, int(64 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(128 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(128 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(256 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(256 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(512 * alpha), depth_multiplier=depth_multiplier)\n",
    "\n",
    "    x = tf.keras.layers.ZeroPadding2D()(x)\n",
    "    x = depthwise_conv_layer(x, int(1024 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "    x = depthwise_conv_layer(x, int(1024 * alpha), 2, depth_multiplier=depth_multiplier)\n",
    "\n",
    "#   This are the layer which has been used to train imagenet\n",
    "#   x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#   x = tf.keras.layers.Flatten()(x)\n",
    "#   x = tf.keras.layers.Dense(units= classes, activation=classifier_activation)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8a05a-2dc3-4336-af8b-15549459d7e5",
   "metadata": {},
   "source": [
    "# Building a model out of it\n",
    "This just prvovides a sequence of layers (a quite deep neural network, maybe you need to train on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84461c55-f640-4686-9788-e4a56c74cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: c:\\Users\\Klein\\Desktop\\Labcourse\\Lab_Course_Dataset\n",
      "Processed path: c:\\Users\\Klein\\Desktop\\Labcourse\\Lab_Course_Dataset_Processed\n"
     ]
    }
   ],
   "source": [
    "# Get script directory for WSL-compatible paths\n",
    "script_dir = Path(__file__).parent.resolve() if '__file__' in dir() else Path.cwd()\n",
    "base_path = script_dir.parent  # Go up to Labcourse folder\n",
    "\n",
    "# Dataset paths (WSL compatible)\n",
    "dataset_path = base_path / \"Lab_Course_Dataset\"\n",
    "processed_path = base_path / \"Lab_Course_Dataset_Processed\"\n",
    "\n",
    "cfg = {\n",
    "    \"dataset\": \"Lab_Course_Dataset\",\n",
    "    \"img_input_shape\": [224, 224, 3],\n",
    "    \"number_of_classes\": 0,  # Will be auto-detected\n",
    "    \"lr1\": 1e-4,\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_path\": str(dataset_path),\n",
    "    \"processed_path\": str(processed_path)\n",
    "}\n",
    "\n",
    "print(f\"Dataset path: {cfg['data_path']}\")\n",
    "print(f\"Processed path: {cfg['processed_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951477db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Found 13 rooms\n",
      "  HW_716: 208 images\n",
      "  HW_718: 177 images\n",
      "  HW_719: 182 images\n",
      "  RM_126: 192 images\n",
      "  HW_706: 225 images\n",
      "  HW_708: 235 images\n",
      "  HW_709: 350 images\n",
      "  RM_001: 211 images\n",
      "  RM_006: 167 images\n",
      "  RM_008: 142 images\n",
      "  RM_012: 189 images\n",
      "  RM_020: 237 images\n",
      "  RM_030: 263 images\n",
      "Dataset created at c:\\Users\\Klein\\Desktop\\Labcourse\\Lab_Course_Dataset_Processed\n",
      "Found 13 classes: ['HW_706', 'HW_708', 'HW_709', 'HW_716', 'HW_718', 'HW_719', 'RM_001', 'RM_006', 'RM_008', 'RM_012', 'RM_020', 'RM_030', 'RM_126']\n"
     ]
    }
   ],
   "source": [
    "# Function to collect all images from the nested folder structure\n",
    "def collect_images_from_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Traverse the Lab_Course_Dataset structure and collect all images organized by location.\n",
    "    Returns a dict: {location_name: [list of image paths]}\n",
    "    \"\"\"\n",
    "    images_by_location = {}\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Traverse the nested structure\n",
    "    for building in dataset_path.iterdir():\n",
    "        if not building.is_dir():\n",
    "            continue\n",
    "        # Handle double-nested folder (Lise-Meitner-Str-9_9377/Lise-Meitner-Str-9_9377/)\n",
    "        inner_building = building / building.name\n",
    "        if inner_building.exists():\n",
    "            building = inner_building\n",
    "        \n",
    "        # Check both Indoor and Outdoor folders\n",
    "        for category in [\"Indoor\", \"Outdoor\"]:\n",
    "            category_path = building / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "            \n",
    "            if category == \"Indoor\":\n",
    "                # Indoor has floor structure\n",
    "                for floor in category_path.iterdir():\n",
    "                    if not floor.is_dir():\n",
    "                        continue\n",
    "                    for location in floor.iterdir():\n",
    "                        if not location.is_dir():\n",
    "                            continue\n",
    "                        location_name = location.name\n",
    "                        if location_name not in images_by_location:\n",
    "                            images_by_location[location_name] = []\n",
    "                        # Collect all image files\n",
    "                        for img_file in location.iterdir():\n",
    "                            if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                                images_by_location[location_name].append(img_file)\n",
    "            else:\n",
    "                # Outdoor has direct location folders\n",
    "                for location in category_path.iterdir():\n",
    "                    if not location.is_dir():\n",
    "                        continue\n",
    "                    location_name = location.name\n",
    "                    if location_name not in images_by_location:\n",
    "                        images_by_location[location_name] = []\n",
    "                    # Collect all image files\n",
    "                    for img_file in location.iterdir():\n",
    "                        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                            images_by_location[location_name].append(img_file)\n",
    "    \n",
    "    return images_by_location\n",
    "\n",
    "# Function to create train/val/test split\n",
    "def create_dataset_splits(images_by_location, output_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Create train/val/test directory structure with proper splits.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Create directories\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for location_name in images_by_location.keys():\n",
    "            (output_path / split / location_name).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Split and copy images\n",
    "    total_images = 0\n",
    "    for location_name, images in images_by_location.items():\n",
    "        if len(images) < 3:\n",
    "            print(f\"Skipping {location_name}: not enough images ({len(images)})\")\n",
    "            continue\n",
    "            \n",
    "        # Split images\n",
    "        train_imgs, temp_imgs = train_test_split(images, train_size=train_ratio, random_state=42)\n",
    "        relative_val = val_ratio / (val_ratio + test_ratio)\n",
    "        val_imgs, test_imgs = train_test_split(temp_imgs, train_size=relative_val, random_state=42)\n",
    "        \n",
    "        # Copy images\n",
    "        for img in train_imgs:\n",
    "            shutil.copy2(img, output_path / 'train' / location_name / img.name)\n",
    "        for img in val_imgs:\n",
    "            shutil.copy2(img, output_path / 'val' / location_name / img.name)\n",
    "        for img in test_imgs:\n",
    "            shutil.copy2(img, output_path / 'test' / location_name / img.name)\n",
    "        \n",
    "        total_images += len(images)\n",
    "        print(f\"  {location_name}: {len(images)} images -> train:{len(train_imgs)}, val:{len(val_imgs)}, test:{len(test_imgs)}\")\n",
    "    \n",
    "    print(f\"\\nDataset created at {output_path}\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "\n",
    "# Check if processed dataset exists, if not create it\n",
    "if not os.path.exists(cfg[\"processed_path\"]) or not os.path.exists(os.path.join(cfg[\"processed_path\"], \"train\")):\n",
    "    print(\"Processing dataset...\")\n",
    "    images_by_location = collect_images_from_dataset(cfg[\"data_path\"])\n",
    "    print(f\"Found {len(images_by_location)} locations\")\n",
    "    create_dataset_splits(images_by_location, cfg[\"processed_path\"])\n",
    "else:\n",
    "    print(\"Using existing processed dataset\")\n",
    "\n",
    "# Auto-detect number of classes\n",
    "train_path = os.path.join(cfg[\"processed_path\"], \"train\")\n",
    "if os.path.exists(train_path):\n",
    "    classes = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "    cfg[\"number_of_classes\"] = len(classes)\n",
    "    print(f\"\\nFound {cfg['number_of_classes']} classes: {sorted(classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e89f1b-567a-4e63-a0df-76444b48b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5722 images belonging to 2 classes.\n",
      "Found 2729 images belonging to 2 classes.\n",
      "Found 2492 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Temp\\ipykernel_17008\\4282542236.py:58: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "178/178 [==============================] - 459s 2s/step - loss: 0.2217 - accuracy: 0.9125 - val_loss: 1.1922 - val_accuracy: 0.5121\n",
      "Epoch 2/15\n",
      "178/178 [==============================] - 378s 2s/step - loss: 0.0983 - accuracy: 0.9652 - val_loss: 2.4825 - val_accuracy: 0.5118\n",
      "Epoch 3/15\n",
      "178/178 [==============================] - 323s 2s/step - loss: 0.0710 - accuracy: 0.9763 - val_loss: 2.7862 - val_accuracy: 0.5136\n",
      "Epoch 4/15\n",
      "178/178 [==============================] - 318s 2s/step - loss: 0.0525 - accuracy: 0.9830 - val_loss: 1.1160 - val_accuracy: 0.5919\n",
      "Epoch 5/15\n",
      "178/178 [==============================] - 301s 2s/step - loss: 0.0474 - accuracy: 0.9837 - val_loss: 0.0739 - val_accuracy: 0.9768\n",
      "Epoch 6/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0393 - accuracy: 0.9854 - val_loss: 0.0950 - val_accuracy: 0.9761\n",
      "Epoch 7/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.0938 - val_accuracy: 0.9732\n",
      "Epoch 8/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.1110 - val_accuracy: 0.9735\n",
      "Epoch 9/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0210 - accuracy: 0.9926 - val_loss: 0.1095 - val_accuracy: 0.9735\n",
      "Epoch 10/15\n",
      "178/178 [==============================] - 283s 2s/step - loss: 0.0197 - accuracy: 0.9926 - val_loss: 0.1074 - val_accuracy: 0.9695\n",
      "Epoch 11/15\n",
      "178/178 [==============================] - 280s 2s/step - loss: 0.0159 - accuracy: 0.9937 - val_loss: 0.1607 - val_accuracy: 0.9710\n",
      "Epoch 12/15\n",
      "178/178 [==============================] - 282s 2s/step - loss: 0.0143 - accuracy: 0.9930 - val_loss: 0.1161 - val_accuracy: 0.9754\n",
      "Epoch 13/15\n",
      "178/178 [==============================] - 999s 6s/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.1978 - val_accuracy: 0.9618\n",
      "Epoch 14/15\n",
      "178/178 [==============================] - 383s 2s/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
      "Epoch 15/15\n",
      "178/178 [==============================] - 384s 2s/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.2024 - val_accuracy: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "new_inputs = tf.keras.layers.Input(shape=cfg[\"img_input_shape\"])\n",
    "x = mobilenet(new_inputs)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "new_outputs = tf.keras.layers.Dense(cfg[\"number_of_classes\"], activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(new_inputs, new_outputs)\n",
    "\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=cfg[\"lr1\"])\n",
    "    \n",
    "use_metrics = ['accuracy']\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=use_metrics)\n",
    "print(model.summary())\n",
    "\n",
    "# Data generators with augmentation\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=os.path.join(cfg[\"processed_path\"], \"train\"),\n",
    "    target_size=(cfg[\"img_input_shape\"][0], cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory=os.path.join(cfg[\"processed_path\"], \"val\"),\n",
    "    target_size=(cfg[\"img_input_shape\"][0], cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = valid_datagen.flow_from_directory(\n",
    "    directory=os.path.join(cfg[\"processed_path\"], \"test\"),\n",
    "    target_size=(cfg[\"img_input_shape\"][0], cfg[\"img_input_shape\"][1]),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
    "\n",
    "# Training with callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TerminateOnNaN(),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=cfg[\"epochs\"],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model with absolute path\n",
    "model_path = base_path / \"mobilenet-lab_course_dataset.h5\"\n",
    "model.save(str(model_path))\n",
    "print(f\"Model saved as '{model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1546c2-ac71-4ea7-a082-198491b61588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 61s 776ms/step - loss: 0.4432 - accuracy: 0.9362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44316813349723816, 0.9361958503723145]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "history_path = base_path / \"training_history.png\"\n",
    "plt.savefig(str(history_path))\n",
    "print(f\"Training history saved to '{history_path}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b70ed0-a9dd-4268-b88a-eae06fd8ad23",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The MobileNet architecture provides an architecture deployable to small devices. With GPU acceleration via WSL, training is significantly faster. The model can classify rooms from the Lab_Course_Dataset effectively, handling the complex nested folder structure and organizing data into proper train/val/test splits automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd657de-0cf9-4115-bac9-c5b5f1dd4e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
