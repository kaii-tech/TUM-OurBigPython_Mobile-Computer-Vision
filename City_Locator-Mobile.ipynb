{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3bce9d",
   "metadata": {},
   "source": [
    "# City Locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcd8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/go56pic/miniconda3/envs/tf-linux/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/go56pic/.cache/kagglehub/datasets/amaralibey/gsv-cities/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"amaralibey/gsv-cities\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe78619",
   "metadata": {},
   "source": [
    "## City Locator (BIG Model)\n",
    "\n",
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabc5017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 19:17:31.176515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Enable mixed precision for speed and lower memory when supported\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f03e78",
   "metadata": {},
   "source": [
    "### Load Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data directory: /home/go56pic/.cache/kagglehub/datasets/amaralibey/gsv-cities/versions/1/Images\n",
      "Found 23 class folders: ['Miami', 'Medellin', 'London', 'Brussels', 'Barcelona', 'Lisbon', 'Melbourne', 'Bangkok', 'Rome', 'Chicago']...\n",
      "Found 529506 files belonging to 23 classes.\n",
      "Using 423605 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1768501058.288554   31885 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1481 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 529506 files belonging to 23 classes.\n",
      "Using 105901 files for validation.\n",
      "Detected NUM_CLASSES = 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "DATA_ROOT = Path(path)\n",
    "\n",
    "# Recursively search for a directory that contains many class subfolders with files\n",
    "def find_class_dir(root: Path, max_depth: int = 3, min_classes: int = 3):\n",
    "    queue = [(root, 0)]\n",
    "    best = None\n",
    "    while queue:\n",
    "        current, depth = queue.pop(0)\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "        subdirs = [d for d in current.iterdir() if d.is_dir()]\n",
    "        if len(subdirs) >= min_classes:\n",
    "            has_files = any(any(f.is_file() for f in d.iterdir()) for d in subdirs)\n",
    "            if has_files:\n",
    "                return current, subdirs\n",
    "            best = best or (current, subdirs)\n",
    "        for sd in subdirs:\n",
    "            queue.append((sd, depth + 1))\n",
    "    return best if best else (root, [d for d in root.iterdir() if d.is_dir()])\n",
    "\n",
    "DATA_DIR, class_dirs = find_class_dir(DATA_ROOT)\n",
    "print(f\"Using data directory: {DATA_DIR}\")\n",
    "if class_dirs:\n",
    "    preview = [d.name for d in class_dirs][:10]\n",
    "    suffix = \"...\" if len(class_dirs) > 10 else \"\"\n",
    "    print(f\"Found {len(class_dirs)} class folders: {preview}{suffix}\")\n",
    "else:\n",
    "    print(\"Warning: No class subfolders found; please verify dataset structure.\")\n",
    "\n",
    "IMG_SIZE = 224  # smaller for faster MobileNetV2 training\n",
    "BATCH_SIZE = 128  # lower if you hit OOM; raise if GPU allows\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(train_ds.class_names)\n",
    "print(f\"Detected NUM_CLASSES = {NUM_CLASSES}\")\n",
    "\n",
    "# Pipeline optimizations\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6fceb",
   "metadata": {},
   "source": [
    "### Inspect Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45692103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes detected: 23\n",
      "Miami: 43637 images\n",
      "Medellin: 6024 images\n",
      "London: 58672 images\n",
      "Brussels: 14171 images\n",
      "Barcelona: 15894 images\n",
      "Lisbon: 27045 images\n",
      "Melbourne: 28542 images\n",
      "Bangkok: 22271 images\n",
      "Rome: 24068 images\n",
      "Chicago: 34091 images\n",
      "Osaka: 22605 images\n",
      "Minneapolis: 22326 images\n",
      "Madrid: 14554 images\n",
      "WashingtonDC: 11545 images\n",
      "MexicoCity: 12801 images\n",
      "Boston: 32616 images\n",
      "BuenosAires: 8481 images\n",
      "PRG: 17590 images\n",
      "LosAngeles: 8891 images\n",
      "PRS: 39963 images\n",
      "Phoenix: 36251 images\n",
      "OSL: 9756 images\n",
      "TRT: 17712 images\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Summarize class counts (first 30 shown)\n",
    "if class_dirs:\n",
    "    counts = {}\n",
    "    for d in class_dirs:\n",
    "        counts[d.name] = sum(1 for f in d.rglob(\"*\") if f.is_file())\n",
    "    print(f\"Total classes detected: {len(counts)}\")\n",
    "    top_names = list(counts.keys())[:30]\n",
    "    for name in top_names:\n",
    "        print(f\"{name}: {counts[name]} images\")\n",
    "    if len(counts) > 30:\n",
    "        print(f\"... {len(counts) - 30} more classes not shown\")\n",
    "else:\n",
    "    print(\"No classes detected; please inspect the dataset root manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb0e81",
   "metadata": {},
   "source": [
    "### Add Noise / Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ec481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation tuned for location invariance\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.12),\n",
    "        layers.RandomZoom(0.15),\n",
    "        layers.RandomTranslation(0.1, 0.1),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d464da0",
   "metadata": {},
   "source": [
    "### Setup and Build Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef66dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Base model: MobileNetV2 for speed/size\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    alpha=1.0,\n",
    ")\n",
    "\n",
    "# Stage 1: freeze backbone\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_avg_pool\")(x)\n",
    "\n",
    "# Lightweight classification head\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"mobilenetv2_city_locator\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top_5_acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3768694",
   "metadata": {},
   "source": [
    "### Execute Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6546222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mobilenetv2_city_locator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mobilenetv2_city_locator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,911</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m327,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ predictions (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │         \u001b[38;5;34m5,911\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,591,831</span> (9.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,591,831\u001b[0m (9.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,847</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m333,847\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 19:18:07.131192: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 522/6619\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:32\u001b[0m 74ms/step - accuracy: 0.1617 - loss: 3.0250 - top_5_acc: 0.4540"
     ]
    }
   ],
   "source": [
    "# Show the model structure and run a short training loop so the cell produces output\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=6,  # shorter for MobileNetV2\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82ce09",
   "metadata": {},
   "source": [
    "### Plot Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"loss\"])\n",
    "plt.title(\"Train Loss Curve\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.epoch, history.history[\"accuracy\"])\n",
    "plt.title(\"Train Accuracy Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83cf6e",
   "metadata": {},
   "source": [
    "### Fine-Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze top blocks of MobileNetV2 for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "fine_tune_at = int(len(base_model.layers) * 0.8)\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = i >= fine_tune_at\n",
    "    if isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top_5_acc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train again with early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
